{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This notebook demonstrates the data pipeline from raw tables to analytical datasets. At the end of this activity, train & test data sets are created from raw data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import os\n",
    "import os.path as op\n",
    "import shutil\n",
    "\n",
    "# standard third party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "pd.options.mode.use_inf_as_na = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard code-template imports\n",
    "from ta_lib.core.api import (\n",
    "    create_context, get_dataframe, get_feature_names_from_column_transformer, get_package_path,\n",
    "    display_as_tabs, string_cleaning, merge_info, initialize_environment,\n",
    "    list_datasets, load_dataset, save_dataset\n",
    ")\n",
    "import ta_lib.eda.api as eda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialize_environment(debug=False, hide_warnings=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/raw/housing',\n",
      " '/cleaned/housing',\n",
      " '/processed/housing',\n",
      " '/train/housing/features',\n",
      " '/train/housing/target',\n",
      " '/test/housing/features',\n",
      " '/test/housing/target',\n",
      " '/score/housing/output']\n"
     ]
    }
   ],
   "source": [
    "config_path = op.join('conf', 'config.yml')\n",
    "context = create_context(config_path)\n",
    "pprint(list_datasets(context))\n",
    "\n",
    "housing_df = load_dataset(context, 'raw/housing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data cleaning and consolidation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<u>NOTES</u>**\n",
    "\n",
    "The focus here is to create a cleaned dataset that is appropriate for solving the DS problem at hand from the raw data.\n",
    "\n",
    "**1. Do**\n",
    "* clean dataframe column names\n",
    "* ensure dtypes are set properly\n",
    "* join with other tables etc to create features\n",
    "* transform, if appropriate, datetime like columns to generate additional features (weekday etc)\n",
    "* transform, if appropriate, string columns to generate additional features\n",
    "* discard cols that are not useful for training the model (IDs, constant cols, duplicate cols etc)\n",
    "* additional features generated from existing columns\n",
    "\n",
    "\n",
    "**2. Don't**\n",
    "* handle missing values or outliers here. mark them and leave them for processing downstream.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Clean tables "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Housing Table\n",
    "\n",
    "From data discovery, we know the following\n",
    "\n",
    "* key columns: None\n",
    "* Numerical columns: longitude, latitude, housing_median_age, total_rooms, total_bedrooms, population, households, median_income, median_house_value\n",
    "* This will go into production code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "      <th>ocean_proximity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-122.23</td>\n",
       "      <td>37.88</td>\n",
       "      <td>41.0</td>\n",
       "      <td>880.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>322.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>8.3252</td>\n",
       "      <td>452600.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-122.22</td>\n",
       "      <td>37.86</td>\n",
       "      <td>21.0</td>\n",
       "      <td>7099.0</td>\n",
       "      <td>1106.0</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>8.3014</td>\n",
       "      <td>358500.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-122.24</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1467.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>496.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>7.2574</td>\n",
       "      <td>352100.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1274.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>558.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>5.6431</td>\n",
       "      <td>341300.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1627.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>259.0</td>\n",
       "      <td>3.8462</td>\n",
       "      <td>342200.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "0    -122.23     37.88                41.0        880.0           129.0   \n",
       "1    -122.22     37.86                21.0       7099.0          1106.0   \n",
       "2    -122.24     37.85                52.0       1467.0           190.0   \n",
       "3    -122.25     37.85                52.0       1274.0           235.0   \n",
       "4    -122.25     37.85                52.0       1627.0           280.0   \n",
       "\n",
       "   population  households  median_income  median_house_value ocean_proximity  \n",
       "0       322.0       126.0         8.3252            452600.0        NEAR BAY  \n",
       "1      2401.0      1138.0         8.3014            358500.0        NEAR BAY  \n",
       "2       496.0       177.0         7.2574            352100.0        NEAR BAY  \n",
       "3       558.0       219.0         5.6431            341300.0        NEAR BAY  \n",
       "4       565.0       259.0         3.8462            342200.0        NEAR BAY  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str_cols = list(\n",
    "    set(housing_df.select_dtypes(\"object\").columns.to_list())\n",
    "    - set(\n",
    "        [\"ocean_proximity\"]\n",
    "    )\n",
    ")\n",
    "housing_df_clean = (\n",
    "    housing_df\n",
    "    # while iterating on testing, it's good to copy the dataset(or a subset)\n",
    "    # as the following steps will mutate the input dataframe. The copy should be\n",
    "    # removed in the production code to avoid introducing perf. bottlenecks.\n",
    "    .copy()\n",
    "\n",
    "    # set dtypes : nothing to do here\n",
    "    .passthrough()\n",
    "    \n",
    "    .transform_columns(str_cols, string_cleaning, elementwise=False)\n",
    "    \n",
    "    .replace({'': np.NaN})\n",
    "    \n",
    "    # clean column names (comment out this line while cleaning data above)\n",
    "    .clean_names(case_type='snake')\n",
    ")\n",
    "housing_df_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE\n",
    "\n",
    "It's always a good idea to save cleaned tabular data using a storage format that supports the following \n",
    "\n",
    "1. preserves the type information\n",
    "2. language agnostic storage format\n",
    "3. Supports compression\n",
    "4. Supports customizing storage to optimize different data access patterns\n",
    "\n",
    "For larger datasets, the last two points become crucial.\n",
    "\n",
    "`Parquet` is one such file format that is very popular for storing tabular data. It has some nice properties:\n",
    "- Similar to pickles & RDS datasets, but compatible with all languages\n",
    "- Preserves the datatypes\n",
    "- Compresses the data and reduces the filesize\n",
    "- Good library support in Python and other languages\n",
    "- As a columnar storage we can efficiently read fewer columns\n",
    "- It also supports chunking data by groups of columns (for instance, by dates or a particular value of a key column) that makes loading subsets of the data fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dataset(context, housing_df_clean, 'cleaned/housing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Generate Train, Validation and Test datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We split the data into train, test (optionally, also a validation dataset)\n",
    "- In this example, we are binning the target into 10 quantiles and then use a Stratified Shuffle to split the data.\n",
    "- See sklearn documentation on the various available splitters\n",
    "- https://scikit-learn.org/stable/modules/classes.html#splitter-classes\n",
    "- This will go into production code (training only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from ta_lib.core.api import custom_train_test_split  # helper function to customize splitting\n",
    "from scripts import *\n",
    "\n",
    "splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=context.random_seed)\n",
    "sales_df_train, sales_df_test = custom_train_test_split(sales_df_processed, splitter, by=binned_median_house_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = \"median_house_value\"\n",
    "\n",
    "train_X, train_y = (\n",
    "    sales_df_train\n",
    "    \n",
    "    # split the dataset to train and test\n",
    "    .get_features_targets(target_column_names=target_col)\n",
    ")\n",
    "save_dataset(context, train_X, 'train/sales/features')\n",
    "save_dataset(context, train_y, 'train/sales/target')\n",
    "\n",
    "\n",
    "test_X, test_y = (\n",
    "    sales_df_test\n",
    "    \n",
    "    # split the dataset to train and test\n",
    "    .get_features_targets(target_column_names=target_col)\n",
    ")\n",
    "save_dataset(context, test_X, 'test/sales/features')\n",
    "save_dataset(context, test_y, 'test/sales/target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
